{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid method: Recursive feature elimination\n",
    "\n",
    "The approach consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance or  coefficients derived from linear models.\n",
    "\n",
    "2) Remove one feature -the least important- and build a machine learning algorithm utilising the remaining features.\n",
    "\n",
    "3) Calculate a performance metric of choice: roc-auc, mse, rmse, accuracy, etc.\n",
    "\n",
    "4) If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "5) Repeat steps 2-4 until all features have been evaluated.\n",
    "\n",
    "\n",
    "I call this a hybrid method because:\n",
    "\n",
    "- it derives the importance derived from the machine learning algorithm, like embedded methods\n",
    "- it builds several machine learning models, like wrapper methods.\n",
    "\n",
    "This method is faster than wrapper methods and often better than embedded methods. In practice it works extremely well. \n",
    "\n",
    "One thing to note is that the minimum drop in performance to decide if a feature should be kept is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
    "\n",
    "I will demonstrate how to select features using this method on a regression and classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_292</th>\n",
       "      <th>var_293</th>\n",
       "      <th>var_294</th>\n",
       "      <th>var_295</th>\n",
       "      <th>var_296</th>\n",
       "      <th>var_297</th>\n",
       "      <th>var_298</th>\n",
       "      <th>var_299</th>\n",
       "      <th>var_300</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67772.7216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_1  var_2  var_3  var_4  var_5  var_6  var_7  var_8  var_9  var_10  ...  \\\n",
       "0      0      0    0.0   0.00    0.0      0      0      0      0       0  ...   \n",
       "1      0      0    0.0   3.00    0.0      0      0      0      0       0  ...   \n",
       "2      0      0    0.0   5.88    0.0      0      0      0      0       0  ...   \n",
       "3      0      0    0.0  14.10    0.0      0      0      0      0       0  ...   \n",
       "4      0      0    0.0   5.76    0.0      0      0      0      0       0  ...   \n",
       "\n",
       "   var_292  var_293  var_294  var_295  var_296  var_297  var_298  var_299  \\\n",
       "0      0.0        0        0        0        0        0        0      0.0   \n",
       "1      0.0        0        0        0        0        0        0      0.0   \n",
       "2      0.0        0        0        3        0        0        0      0.0   \n",
       "3      0.0        0        0        0        0        0        0      0.0   \n",
       "4      0.0        0        0        0        0        0        0      0.0   \n",
       "\n",
       "      var_300  target  \n",
       "0      0.0000       0  \n",
       "1      0.0000       0  \n",
       "2  67772.7216       0  \n",
       "3      0.0000       0  \n",
       "4      0.0000       0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant and quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 158), (15000, 158))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to speed things up we remove constant, quasi-constand and duplicated features\n",
    "\n",
    "quasi_constant_feat = []\n",
    "\n",
    "# iterate over every feature\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    # find the predominant value, that is the value that is shared\n",
    "    # by most observations\n",
    "    predominant = (X_train[feature].value_counts() / np.float(\n",
    "        len(X_train))).sort_values(ascending=False).values[0]\n",
    "\n",
    "    # evaluate the predominant feature: do more than 99% of the observations\n",
    "    # show 1 value?\n",
    "    if predominant > 0.998:\n",
    "        \n",
    "        # if yes, add the variable to the list\n",
    "        quasi_constant_feat.append(feature)\n",
    "\n",
    "X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 152), (15000, 152))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated features\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ML model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC=0.827532\n"
     ]
    }
   ],
   "source": [
    "# the first step of this procedure consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# build initial model using all the features\n",
    "model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "model_full.fit(X_train, y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_full.predict_proba(X_test)[:, 1]\n",
    "roc_full = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print('Test ROC AUC=%f' % (roc_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank features by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the least to the most\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_full.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(20,6))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_154',\n",
       " 'var_203',\n",
       " 'var_200',\n",
       " 'var_198',\n",
       " 'var_194',\n",
       " 'var_193',\n",
       " 'var_192',\n",
       " 'var_191',\n",
       " 'var_188',\n",
       " 'var_181',\n",
       " 'var_179',\n",
       " 'var_177',\n",
       " 'var_176',\n",
       " 'var_175',\n",
       " 'var_174',\n",
       " 'var_205',\n",
       " 'var_172',\n",
       " 'var_168',\n",
       " 'var_165',\n",
       " 'var_164',\n",
       " 'var_163',\n",
       " 'var_162',\n",
       " 'var_161',\n",
       " 'var_160',\n",
       " 'var_156',\n",
       " 'var_155',\n",
       " 'var_295',\n",
       " 'var_152',\n",
       " 'var_147',\n",
       " 'var_144',\n",
       " 'var_143',\n",
       " 'var_169',\n",
       " 'var_140',\n",
       " 'var_206',\n",
       " 'var_209',\n",
       " 'var_293',\n",
       " 'var_292',\n",
       " 'var_288',\n",
       " 'var_284',\n",
       " 'var_281',\n",
       " 'var_279',\n",
       " 'var_278',\n",
       " 'var_275',\n",
       " 'var_273',\n",
       " 'var_270',\n",
       " 'var_268',\n",
       " 'var_266',\n",
       " 'var_262',\n",
       " 'var_261',\n",
       " 'var_207',\n",
       " 'var_259',\n",
       " 'var_256',\n",
       " 'var_255',\n",
       " 'var_253',\n",
       " 'var_252',\n",
       " 'var_242',\n",
       " 'var_241',\n",
       " 'var_238',\n",
       " 'var_230',\n",
       " 'var_229',\n",
       " 'var_226',\n",
       " 'var_220',\n",
       " 'var_218',\n",
       " 'var_214',\n",
       " 'var_213',\n",
       " 'var_258',\n",
       " 'var_139',\n",
       " 'var_300',\n",
       " 'var_134',\n",
       " 'var_82',\n",
       " 'var_79',\n",
       " 'var_76',\n",
       " 'var_70',\n",
       " 'var_68',\n",
       " 'var_137',\n",
       " 'var_63',\n",
       " 'var_62',\n",
       " 'var_58',\n",
       " 'var_57',\n",
       " 'var_55',\n",
       " 'var_54',\n",
       " 'var_52',\n",
       " 'var_51',\n",
       " 'var_47',\n",
       " 'var_41',\n",
       " 'var_38',\n",
       " 'var_31',\n",
       " 'var_30',\n",
       " 'var_27',\n",
       " 'var_26',\n",
       " 'var_25',\n",
       " 'var_22',\n",
       " 'var_19',\n",
       " 'var_15',\n",
       " 'var_13',\n",
       " 'var_5',\n",
       " 'var_83',\n",
       " 'var_84',\n",
       " 'var_64',\n",
       " 'var_108',\n",
       " 'var_123',\n",
       " 'var_128',\n",
       " 'var_114',\n",
       " 'var_117',\n",
       " 'var_103',\n",
       " 'var_109',\n",
       " 'var_131',\n",
       " 'var_118',\n",
       " 'var_96',\n",
       " 'var_100',\n",
       " 'var_93',\n",
       " 'var_119',\n",
       " 'var_88',\n",
       " 'var_86',\n",
       " 'var_94',\n",
       " 'var_85',\n",
       " 'var_272',\n",
       " 'var_74',\n",
       " 'var_91',\n",
       " 'var_145',\n",
       " 'var_231',\n",
       " 'var_277',\n",
       " 'var_17',\n",
       " 'var_101',\n",
       " 'var_29',\n",
       " 'var_271',\n",
       " 'var_244',\n",
       " 'var_208',\n",
       " 'var_166',\n",
       " 'var_37',\n",
       " 'var_75',\n",
       " 'var_240',\n",
       " 'var_276',\n",
       " 'var_121',\n",
       " 'var_8',\n",
       " 'var_50',\n",
       " 'var_21',\n",
       " 'var_157',\n",
       " 'var_105',\n",
       " 'var_186',\n",
       " 'var_222',\n",
       " 'var_4',\n",
       " 'var_18',\n",
       " 'var_107',\n",
       " 'var_190',\n",
       " 'var_185',\n",
       " 'var_173',\n",
       " 'var_49',\n",
       " 'var_132',\n",
       " 'var_46',\n",
       " 'var_35',\n",
       " 'var_110']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make list of ordered features\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature elimination\n",
      "\n",
      "testing feature:  var_154 1  out of  152\n",
      "New Test ROC AUC=0.8275385663137131\n",
      "Full dataset ROC AUC=0.8275318972167823\n",
      "Drop in ROC AUC=-6.669096930811413e-06\n",
      "remove:  var_154\n",
      "\n",
      "testing feature:  var_203 2  out of  152\n",
      "New Test ROC AUC=0.8276405243718757\n",
      "Full dataset ROC AUC=0.8275385663137131\n",
      "Drop in ROC AUC=-0.00010195805816259895\n",
      "remove:  var_203\n",
      "\n",
      "testing feature:  var_200 3  out of  152\n",
      "New Test ROC AUC=0.8274994560164579\n",
      "Full dataset ROC AUC=0.8276405243718757\n",
      "Drop in ROC AUC=0.00014106835541782825\n",
      "remove:  var_200\n",
      "\n",
      "testing feature:  var_198 4  out of  152\n",
      "New Test ROC AUC=0.8275510002232452\n",
      "Full dataset ROC AUC=0.8274994560164579\n",
      "Drop in ROC AUC=-5.1544206787323965e-05\n",
      "remove:  var_198\n",
      "\n",
      "testing feature:  var_194 5  out of  152\n",
      "New Test ROC AUC=0.8275221761602394\n",
      "Full dataset ROC AUC=0.8275510002232452\n",
      "Drop in ROC AUC=2.8824063005861156e-05\n",
      "remove:  var_194\n",
      "\n",
      "testing feature:  var_193 6  out of  152\n",
      "New Test ROC AUC=0.8275526957563633\n",
      "Full dataset ROC AUC=0.8275221761602394\n",
      "Drop in ROC AUC=-3.051959612387911e-05\n",
      "remove:  var_193\n",
      "\n",
      "testing feature:  var_192 7  out of  152\n",
      "New Test ROC AUC=0.8274947085237276\n",
      "Full dataset ROC AUC=0.8275526957563633\n",
      "Drop in ROC AUC=5.798723263561456e-05\n",
      "remove:  var_192\n",
      "\n",
      "testing feature:  var_191 8  out of  152\n",
      "New Test ROC AUC=0.827497195305634\n",
      "Full dataset ROC AUC=0.8274947085237276\n",
      "Drop in ROC AUC=-2.486781906396729e-06\n",
      "remove:  var_191\n",
      "\n",
      "testing feature:  var_188 9  out of  152\n",
      "New Test ROC AUC=0.8274553721553899\n",
      "Full dataset ROC AUC=0.827497195305634\n",
      "Drop in ROC AUC=4.1823150244146845e-05\n",
      "remove:  var_188\n",
      "\n",
      "testing feature:  var_181 10  out of  152\n",
      "New Test ROC AUC=0.8274541287644368\n",
      "Full dataset ROC AUC=0.8274553721553899\n",
      "Drop in ROC AUC=1.2433909530873422e-06\n",
      "remove:  var_181\n",
      "\n",
      "testing feature:  var_179 11  out of  152\n"
     ]
    }
   ],
   "source": [
    "# the final step consists in removing one at a time\n",
    "# all the features, from the least to the most\n",
    "# important, and build an model at each round.\n",
    "\n",
    "# once we build the model, we calculate the new roc-auc\n",
    "\n",
    "# if the new roc-auc is smaller than the original one\n",
    "# (with all the features), then the feature that was removed\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature elimination:\n",
    "\n",
    "# first we arbitrarily set the drop in roc-auc\n",
    "# if the drop is below this threshold,\n",
    "# the feature will be removed\n",
    "tol = 0.0005\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should remove\n",
    "features_to_remove = []\n",
    "\n",
    "# set a counter to know where the loop is\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features is this list are ordered\n",
    "# by importance\n",
    "for feature in features:\n",
    "    \n",
    "    print()\n",
    "    print('testing feature: ', feature, count, ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "    # fit model with all variables, minus the feature to be evaluated\n",
    "    # and also minus all features that were deemed to be removed\n",
    "    \n",
    "    # note that features_to_remove will be empty in the first rounds\n",
    "    # but will have features as the loop proceeds\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # make a prediction using the test set\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
    "\n",
    "    # calculate the new roc-auc\n",
    "    roc_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((roc_int)))\n",
    "\n",
    "    # print the original roc-auc with all the features\n",
    "    print('Full dataset ROC AUC={}'.format((roc_full)))\n",
    "\n",
    "    # determine the drop in the roc-auc\n",
    "    diff_roc = roc_full - roc_int\n",
    "\n",
    "    # compare the drop in roc-auc with the tolerance\n",
    "    # we set previously\n",
    "    if diff_roc >= tol:\n",
    "        print('Drop in ROC AUC={}'.format(diff_roc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in ROC AUC={}'.format(diff_roc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # if the drop in the roc is small and we remove the\n",
    "        # feature, we need to set the new roc to the one based on\n",
    "        # the remaining features\n",
    "        roc_full = roc_int\n",
    "        \n",
    "        # and append the feature to remove to the collecting list\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# determine the features to keep (those we won't remove)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))\n",
    "\n",
    "# The simulation will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for comparison, we build a model with the selected features only\n",
    "\n",
    "model_final = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "# fit the model with the selected features\n",
    "model_final.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = model_final.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# calculate roc-auc\n",
    "roc_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (roc_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, the Gradient Boosting model built with 12 features shows a similar performance than the one built the full dataset (you need to scroll up to find this value, we calculated a few cells ago).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "data = pd.read_csv('houseprice.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['Id','SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ML model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the first step of this procedure consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# build initial model using all the features\n",
    "model_full = GradientBoostingRegressor(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "model_full.fit(X_train, y_train)\n",
    "\n",
    "# calculate r2 in the test set\n",
    "y_pred_test = model_full.predict(X_test)\n",
    "r2_full = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print('Test full dataset R2 = %f' % (r2_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank features by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the least to the most\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_full.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(20,6))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make list with ordered features\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the final step consists in removing one at a time\n",
    "# all the features, from the least to the most\n",
    "# important, and build model at each round.\n",
    "\n",
    "# once we build the model, we calculate the new r2\n",
    "# if the new r2 is smaller than the original one\n",
    "# (with all the features), then that feature that was removed\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature elimination:\n",
    "\n",
    "# first we arbitrarily set the drop in r2\n",
    "# if the drop is below this threshold,\n",
    "# the feature will be removed\n",
    "tol = 0.001\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should remove\n",
    "features_to_remove = []\n",
    "\n",
    "# set a counter to know which feature is being evaluated\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features in the list are ordered\n",
    "# by importance\n",
    "for feature in features:\n",
    "    print()\n",
    "    print('testing feature: ', feature, count, ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = GradientBoostingRegressor(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "    # fit model with all variables minus the removed features\n",
    "    # and the feature to be evaluated\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # make a prediction over the test set\n",
    "    y_pred_test = model_int.predict(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))\n",
    "\n",
    "    # calculate the new r2\n",
    "    r2_int = r2_score(y_test, y_pred_test)\n",
    "    print('New Test r2 = {}'.format((r2_int)))\n",
    "\n",
    "    # print the original r2 with all the features\n",
    "    print('All features Test r2 = {}'.format((r2_full)))\n",
    "\n",
    "    # determine the drop in the r2\n",
    "    diff_r2 = r2_full - r2_int\n",
    "\n",
    "    # compare the drop in r2 with the tolerance\n",
    "    # we set previously\n",
    "    if diff_r2 >= tol:\n",
    "        print('Drop in r2 ={}'.format(diff_r2))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in r2 = {}'.format(diff_r2))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # if the drop in the r2 is small and we remove the\n",
    "        # feature, we need to set the new r2 to the one based on\n",
    "        # the remaining features\n",
    "        r2_full = r2_int\n",
    "        \n",
    "        # and append the feature to remove to the collecting list\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# determine the features to keep (those we won't remove)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally let's test the performance of the model \n",
    "# built on the selected features\n",
    "\n",
    "# build initial model\n",
    "model_final = GradientBoostingRegressor(n_estimators=10, max_depth=4, random_state=10)\n",
    "\n",
    "# fit the model with the selected features\n",
    "model_final.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = model_final.predict(X_test[features_to_keep])\n",
    "\n",
    "# calculate roc-auc\n",
    "r2_final = r2_score(y_test, y_pred_test)\n",
    "print('Test selected features r2 = %f' % (r2_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model built with 28 features shows similar or slightly better performance! (you need to scroll up to find this value, we calculated a few cells ago)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
